{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zattacole/anaconda3/envs/zatta/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import DataLoader, DatasetBuilder\n",
    "#from models import *\n",
    "from utils.input import *\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from models import *\n",
    "from model import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\", font_scale=1.5, palette='magma')\n",
    "import matplotlib\n",
    "matplotlib.use('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(path:str='../DATA/speech_commands_v0.02',\n",
    "                   method_spectrum:str='log_mel',\n",
    "                   test_ratio:float=0.15,\n",
    "                   val_ratio:float=0.05,\n",
    "                   batch_size:int=64,\n",
    "                   shuffle_buffer_size:int=1000,\n",
    "                   shuffle:bool=True,\n",
    "                   seed:int=42,\n",
    "                   verbose:int=1):\n",
    "    \"\"\"\n",
    "    Get the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the data.\n",
    "    method_spectrum : str\n",
    "        Method to compute the spectrum.\n",
    "    test_ratio : float\n",
    "        Ratio of the data to be used as test set.\n",
    "    val_ratio : float\n",
    "        Ratio of the data to be used as validation set.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    shuffle_buffer_size : int\n",
    "        Shuffle buffer size.\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the data.\n",
    "    seed : int\n",
    "        Seed for the random number generator.\n",
    "    verbose : int\n",
    "        Verbosity level.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train : tf.data.Dataset\n",
    "        Training dataset.\n",
    "    test : tf.data.Dataset\n",
    "        Test dataset.\n",
    "    val : tf.data.Dataset\n",
    "        Validation dataset.\n",
    "    commands : list\n",
    "        List of commands.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the files.\n",
    "    data = DataLoader(\n",
    "        path=path\n",
    "    )\n",
    "    \n",
    "    commands = data.get_commands()\n",
    "    filenames = data.get_filenames()\n",
    "    train_files, test_files, val_files = data.split_data(\n",
    "        filenames=filenames,\n",
    "        test_ratio=test_ratio,\n",
    "        val_ratio=val_ratio,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    ds = DatasetBuilder(\n",
    "        commands=commands,\n",
    "        train_filenames=train_files,\n",
    "        test_filenames=test_files,\n",
    "        val_filenames=val_files,\n",
    "        batch_size=batch_size,\n",
    "        buffer_size=shuffle_buffer_size,\n",
    "        method=method_spectrum\n",
    "    )\n",
    "    \n",
    "    train, test, val = ds.preprocess_dataset_spectrogram()\n",
    "    return train, test, val, commands\n",
    "\n",
    "def evaluation_pipeline(\n",
    "    model_name:str,\n",
    "    model:tf.keras.Model,\n",
    "    test_ds:tf.data.Dataset,\n",
    "    commands:list,\n",
    "    verbose:int=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        Trained model.\n",
    "    test_ds : tf.data.Dataset\n",
    "        Test dataset.\n",
    "    commands : list\n",
    "        List of commands.\n",
    "    verbose : int\n",
    "        Verbosity level.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Evaluate the model.   \n",
    "    mtest = model.evaluate(\n",
    "            set='test',\n",
    "            method='confusion_matrix',\n",
    "            model_name=model_name,\n",
    "        )\n",
    "       \n",
    "    # Save the metrics.\n",
    "    return mtest\n",
    "\n",
    "import models\n",
    "def training_pipeline(\n",
    "    name_model:str,\n",
    "    train_ds:tf.data.Dataset,\n",
    "    test_ds:tf.data.Dataset,\n",
    "    val_ds:tf.data.Dataset,\n",
    "    commands:list,\n",
    "    loss:str,\n",
    "    optimizer:str,\n",
    "    metrics:str,\n",
    "    epochs:int=300,\n",
    "    use_tensorboard:bool=True,\n",
    "    save_checkpoint:bool=True,\n",
    "    verbose:int=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the model, compile it, train it and evaluate it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the model.\n",
    "    model = getattr(models, name_model)(\n",
    "        train_ds=train_ds,\n",
    "        test_ds=test_ds,\n",
    "        val_ds=val_ds,\n",
    "        commands=commands\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print('Model: {}'.format(name_model))\n",
    "\n",
    "    model.define_model()\n",
    "    model.load_weights('logmel/models/CNNOneTStride8.h5')\n",
    "   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    path='DATA/speech_commands_v0.02',\n",
    "    method_spectrum='STFT',\n",
    "    test_ratio=0.15,\n",
    "    val_ratio=0.05,\n",
    "    batch_size=128,\n",
    "    shuffle_buffer_size=1000,\n",
    "    name_model='CNNOneTStride8',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    lr=0.01,\n",
    "    metrics='accuracy',\n",
    "    epochs=300,\n",
    "    shuffle=True,\n",
    "    use_tensorboard:bool=True,\n",
    "    save_checkpoint:bool=True,\n",
    "    verbose=1,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function. Get the data, train the model and evaluate it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the data.\n",
    "    method_spectrum : str\n",
    "        Method to compute the spectrum.\n",
    "    test_ratio : float\n",
    "        Ratio of the data to be used as test set.\n",
    "    val_ratio : float\n",
    "        Ratio of the data to be used as validation set.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    shuffle_buffer_size : int\n",
    "        Shuffle buffer size.\n",
    "    name_model : str\n",
    "        Name of the model.\n",
    "    loss : str\n",
    "        Loss function.\n",
    "    optimizer : str\n",
    "        Optimizer.\n",
    "    metrics : str\n",
    "        Metrics.\n",
    "    epochs : int\n",
    "        Number of epochs.\n",
    "    seed : int\n",
    "        Seed for the random number generator.\n",
    "    verbose : int\n",
    "        Verbosity level.\n",
    "    \"\"\"\n",
    "\n",
    "    # print args\n",
    "    print('path: {}'.format(path))\n",
    "    print('method_spectrum: {}'.format(method_spectrum))\n",
    "    print('test_ratio: {}'.format(test_ratio))\n",
    "    print('val_ratio: {}'.format(val_ratio))\n",
    "    print('batch_size: {}'.format(batch_size))\n",
    "    print('shuffle_buffer_size: {}'.format(shuffle_buffer_size))\n",
    "    print('name_model: {}'.format(name_model))\n",
    "    print('loss: {}'.format(loss))\n",
    "    print('lr: {}'.format(lr))\n",
    "    print('metrics: {}'.format(metrics))\n",
    "    print('epochs: {}'.format(epochs))\n",
    "    print('shuffle: {}'.format(shuffle))\n",
    "    print('use_tensorboard: {}'.format(use_tensorboard))\n",
    "    print('save_checkpoint: {}'.format(save_checkpoint))\n",
    "\n",
    "\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=float(lr), weight_decay=1e-5)\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=float(lr), decay=1e-5)\n",
    "\n",
    "    train, test, val, commands = input_pipeline(\n",
    "        path=path,\n",
    "        method_spectrum=method_spectrum,\n",
    "        test_ratio=test_ratio,\n",
    "        val_ratio=val_ratio,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer_size=shuffle_buffer_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    # img size\n",
    "    img_size = train.element_spec\n",
    "    print(img_size)\n",
    "\n",
    "    \n",
    "    model = training_pipeline(\n",
    "        name_model=name_model,\n",
    "        train_ds=train,\n",
    "        test_ds=test,\n",
    "        val_ds=val,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        epochs=100,\n",
    "        use_tensorboard=use_tensorboard,\n",
    "        save_checkpoint=save_checkpoint,\n",
    "        verbose=1,\n",
    "        commands=commands,\n",
    "    )\n",
    "\n",
    "    \n",
    "    mat = evaluation_pipeline(\n",
    "        model_name=name_model,\n",
    "        model=model,\n",
    "        test_ds=test,\n",
    "        commands=commands,\n",
    "        verbose=1\n",
    "    )\n",
    "    return mat, commands\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: DATA/speech_commands_v0.02\n",
      "method_spectrum: STFT\n",
      "test_ratio: 0.15\n",
      "val_ratio: 0.05\n",
      "batch_size: 128\n",
      "shuffle_buffer_size: 1000\n",
      "name_model: CNNOneTStride8\n",
      "loss: sparse_categorical_crossentropy\n",
      "lr: 0.01\n",
      "metrics: accuracy\n",
      "epochs: 300\n",
      "shuffle: True\n",
      "use_tensorboard: True\n",
      "save_checkpoint: True\n",
      "Dataset path:  DATA/speech_commands_v0.02\n",
      "(TensorSpec(shape=(None, 257, None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 17:13:15.389387: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CNNOneTStride8\n",
      "42/42 [==============================] - 2s 33ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "m, command = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize rows \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(font_scale=1.5, palette=\"Blues\")\n",
    "m, command = m\n",
    "mat_norm = m/np.sum(m, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(mat_norm, ax=ax, cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "fig.savefig('confusion_matrix.pdf', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zatta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
